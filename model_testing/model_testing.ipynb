{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install the needed libraries and packages ##\n",
    "%%capture  # Suppresses the output of the installations\n",
    "\n",
    "!pip install datasets    # Library needed to access the dataset\n",
    "!pip install transformers==4.11.3   # Library needed to access Hugging Face's state-of-the-art pre-trained transformer models\n",
    "!pip install librosa    # Library needed for audio and music processing and analysis\n",
    "!pip install jiwer    # Library needed to calculate the word error rate (WER) of the ASR models\n",
    "!pip install ipywidgets   # Library for interactive widgets in Jupyter notebooks\n",
    "!pip install torch   # Library needed for the PyTorch deep and machine learning framework\n",
    "!pip install scipy   # Library for scientific and technical computing\n",
    "!pip install matplotlib   # Library useful for making graphs and charts\n",
    "!pip install scikit_posthocs   # Library for post hoc tests, useful for statistical analysis after Kruskal Wallis tests for instance\n",
    "!pip install numpy # Library for handling numerical computations, can also be useful for positioning bars on graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the needed libraries and packages ##\n",
    "\n",
    "from datasets import DatasetDict, load_dataset, load_metric, Audio\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro, levene, kruskal\n",
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load each dataset split individually. This needs to be done manually for all the models ##\n",
    "\n",
    "samromur_milljon = load_dataset(\"language-and-voice-lab/samromur_milljon\", split=\"female_lt_18_yrs\")\n",
    "\n",
    "# Female speakers less than 18 years old (n < 18): female_lt_18_yrs \n",
    "\n",
    "# Female speakers from 18 to 49 years old (18 <= n <=49): female_18to49_yrs \n",
    "\n",
    "# Female speakers greater than 49 years old (n > 49): female_gt_49_yrs \n",
    "\n",
    "# Male speakers less than 18 years old (n < 18): male_lt_18_yrs \n",
    "\n",
    "# Male speakers from 18 to 49 years old (18 <= n <=49): male_18to49_yrs \n",
    "\n",
    "# Male speakers greater than 49 years old (n > 49): male_gt_49_yrs \n",
    "\n",
    "# Speakers where age, gender or both are unknown: other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the column names and the number of rows ##\n",
    "samromur_milljon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the dataset into training and test datasets ##\n",
    "split_datasets = samromur_milljon.train_test_split(test_size=0.2, seed=11) # Setting a seed for reproducibility\n",
    "\n",
    "## Further split the training dataset into training and validation datasets ##\n",
    "train_val_datasets = split_datasets[\"train\"].train_test_split(test_size=0.125, seed=11) # Setting the same seed here\n",
    "\n",
    "## Creating a DatasetDict to hold the adjusted splits ##\n",
    "ds = DatasetDict({\n",
    "    \"train\": train_val_datasets[\"train\"], # Use the larger part of the train split for training, 70% of the dataset\n",
    "    \"validation\": split_datasets[\"test\"], # Use the initial test split as validation, 10% of the dataset\n",
    "    \"test\": train_val_datasets[\"test\"] # Use the smaller part of the train split for testing during training, 20% of the dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the content of the train, validation and test splits ##\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the validation dataset split ##\n",
    "\n",
    "validation_dataset = ds['validation']\n",
    "print(\"Validation Dataset Loaded, Records Count:\", len(validation_dataset))\n",
    "\n",
    "## Seed for investigating possible seed randomness and to ensure reproducibility. The seeds have to be commented and uncommented manually for each run ##\n",
    "np.random.seed(42)\n",
    "#np.random.seed(43)\n",
    "#np.random.seed(44)\n",
    "\n",
    "## Shuffle the indices ##\n",
    "indices = np.random.permutation(len(validation_dataset))\n",
    "\n",
    "## Calculate split sizes ##\n",
    "total_count = len(validation_dataset)\n",
    "first_split_size = total_count // 3\n",
    "second_split_size = (total_count - first_split_size) // 2\n",
    "\n",
    "## Create splits ##\n",
    "split1_indices = indices[:first_split_size]\n",
    "split2_indices = indices[first_split_size:first_split_size + second_split_size]\n",
    "split3_indices = indices[first_split_size + second_split_size:]\n",
    "\n",
    "## Create subsets for the test dataset split based on calculated indices ##\n",
    "split1 = validation_dataset.select(split1_indices)\n",
    "split2 = validation_dataset.select(split2_indices)\n",
    "split3 = validation_dataset.select(split3_indices)\n",
    "\n",
    "## Output sizes and total duration of each split ##\n",
    "print(\"Validation Split 1 Sample Size:\", len(split1), \", Total Duration:\", sum(split1['duration']) / 3600, \"hours\")\n",
    "print(\"Validation Split 2 Sample Size:\", len(split2), \", Total Duration:\", sum(split2['duration']) / 3600, \"hours\")\n",
    "print(\"Validation Split 3 Sample Size:\", len(split3), \", Total Duration:\", sum(split3['duration']) / 3600, \"hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code runs once for each seed for each model. Seed has to be changed manually before each run, in the code above ##\n",
    "\n",
    "## Load the processor and model ##\n",
    "MODEL_NAME = \"gudjonk93/female_under_18\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_NAME)\n",
    "\n",
    "## Check if CUDA is available ##\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "## WER, the evaluation metric, trusting remote code for future releases ##\n",
    "wer_metric = load_metric(\"wer\", trust_remote_code=True)\n",
    "\n",
    "## Function to prepare the dataset by processing audio and text data ##\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"normalized_text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "## Function to map results for evaluation ##\n",
    "def map_to_result(batch):\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor([batch[\"input_values\"]], device=device)\n",
    "        logits = model(input_values).logits\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "    batch[\"sentence\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "    return batch\n",
    "\n",
    "## Function to evaluate a given dataset split ##\n",
    "def evaluate_split(split, description):\n",
    "    print(f\"Evaluating {description}\")\n",
    "    split = split.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "    split = split.map(prepare_dataset, remove_columns=split.column_names, num_proc=1)\n",
    "    results = split.map(map_to_result, remove_columns=split.column_names)\n",
    "    all_predictions = [r[\"pred_str\"] for r in results]\n",
    "    all_references = [r[\"sentence\"] for r in results]\n",
    "    wer_score = wer_metric.compute(predictions=all_predictions, references=all_references)\n",
    "    print(f\"{description} WER: {wer_score:.3f}\")\n",
    "\n",
    "## Evaluate each split sequentially ##\n",
    "evaluate_split(split1, \"Split 1\")\n",
    "evaluate_split(split2, \"Split 2\")\n",
    "evaluate_split(split3, \"Split 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the WER scores and groups as a pandas DataFrame. The pandas DataFrame was filled manually, following results from the WER-score testing above ##\n",
    "\n",
    "data = {\n",
    "    'scores': [\n",
    "        0.118, 0.119, 0.118, 0.118, 0.118, 0.118, 0.118, 0.118, 0.119,  # Female under 18, seeds 42, 43, 44\n",
    "        0.084, 0.085, 0.085, 0.084, 0.085, 0.085, 0.085, 0.085, 0.084,  # Female 18-49, seeds 42, 43, 44\n",
    "        0.109, 0.109, 0.112, 0.112, 0.109, 0.108, 0.110, 0.106, 0.112,  # Female over 49, seeds 42, 43, 44\n",
    "        0.150, 0.146, 0.149, 0.150, 0.151, 0.145, 0.151, 0.149, 0.145,  # Male under 18, seeds 42, 43, 44\n",
    "        0.106, 0.103, 0.105, 0.105, 0.104, 0.105, 0.104, 0.107, 0.103,  # Male 18-49, seeds 42, 43, 44\n",
    "        0.155, 0.161, 0.161, 0.156, 0.161, 0.160, 0.159, 0.157, 0.160   # Male over 49, seeds 42, 43, 44\n",
    "    ],\n",
    "    'groups': [\n",
    "        'female_under_18', 'female_under_18', 'female_under_18', 'female_under_18', 'female_under_18', 'female_under_18', 'female_under_18', 'female_under_18', 'female_under_18',\n",
    "        'female_18_to_49', 'female_18_to_49', 'female_18_to_49', 'female_18_to_49', 'female_18_to_49', 'female_18_to_49', 'female_18_to_49', 'female_18_to_49', 'female_18_to_49',\n",
    "        'female_over_49', 'female_over_49', 'female_over_49', 'female_over_49', 'female_over_49', 'female_over_49', 'female_over_49', 'female_over_49', 'female_over_49',\n",
    "        'male_under_18', 'male_under_18', 'male_under_18', 'male_under_18', 'male_under_18', 'male_under_18', 'male_under_18', 'male_under_18', 'male_under_18',\n",
    "        'male_18_to_49', 'male_18_to_49', 'male_18_to_49', 'male_18_to_49', 'male_18_to_49', 'male_18_to_49', 'male_18_to_49', 'male_18_to_49', 'male_18_to_49',\n",
    "        'male_over_49', 'male_over_49', 'male_over_49', 'male_over_49', 'male_over_49', 'male_over_49', 'male_over_49', 'male_over_49', 'male_over_49'\n",
    "    ],\n",
    "    'seeds': [\n",
    "        '42', '42', '42', '43', '43', '43', '44', '44', '44',\n",
    "        '42', '42', '42', '43', '43', '43', '44', '44', '44',\n",
    "        '42', '42', '42', '43', '43', '43', '44', '44', '44',\n",
    "        '42', '42', '42', '43', '43', '43', '44', '44', '44',\n",
    "        '42', '42', '42', '43', '43', '43', '44', '44', '44',\n",
    "        '42', '42', '42', '43', '43', '43', '44', '44', '44'\n",
    "    ],\n",
    "    'rep': [\n",
    "        1, 2, 3, 1, 2, 3, 1, 2, 3,\n",
    "        1, 2, 3, 1, 2, 3, 1, 2, 3,\n",
    "        1, 2, 3, 1, 2, 3, 1, 2, 3,\n",
    "        1, 2, 3, 1, 2, 3, 1, 2, 3,\n",
    "        1, 2, 3, 1, 2, 3, 1, 2, 3,\n",
    "        1, 2, 3, 1, 2, 3, 1, 2, 3\n",
    "    ]\n",
    "}\n",
    "\n",
    "## Convert to DataFrame ##\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Firstly, get the results to a table ##\n",
    "\n",
    "## Pivot the DataFrame to get the desired format for display ##\n",
    "pivot_df = df.pivot_table(index='groups', columns=['seeds', 'rep'], values='scores')\n",
    "\n",
    "## Rename the columns to \"Seed X - Split Y\" ##\n",
    "pivot_df.columns = [f'{seed} - {rep}' for seed, rep in pivot_df.columns]\n",
    "\n",
    "## Calculate the average for each row and add it as a new column ##\n",
    "pivot_df['Avg WER'] = pivot_df.mean(axis=1)\n",
    "\n",
    "## Reorder the rows to be female models first, then male models ##\n",
    "pivot_df = pivot_df.reindex(['female_under_18', 'female_18_to_49', 'female_over_49', 'male_under_18', 'male_18_to_49', 'male_over_49'])\n",
    "\n",
    "## Format all values to three decimal places ##\n",
    "formatted_values = pivot_df.applymap(lambda x: f\"{x:.3f}\")\n",
    "\n",
    "## Add \"Model name\" to the column names ##\n",
    "formatted_values.reset_index(inplace=True)\n",
    "formatted_values.columns = ['Model name'] + list(formatted_values.columns[1:])\n",
    "\n",
    "## Plot the table ##\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "## Create the table with new column headers ##\n",
    "the_table = ax.table(cellText=formatted_values.values, \n",
    "                     colLabels=formatted_values.columns,\n",
    "                     cellLoc='center', \n",
    "                     loc='center')\n",
    "\n",
    "## Adjust the font size and scale ##\n",
    "the_table.auto_set_font_size(False)\n",
    "the_table.set_fontsize(10)\n",
    "\n",
    "## Adjust the width of the columns and the height of the cells ##\n",
    "for (i, j), cell in the_table.get_celld().items():\n",
    "    if j == 0:\n",
    "        cell.set_width(0.12)  # Width of the first column\n",
    "    elif j == len(formatted_values.columns) - 1:\n",
    "        cell.set_width(0.08)  # Width of the Average column\n",
    "    else:\n",
    "        cell.set_width(0.06)  # Width of all other columns\n",
    "    cell.set_height(0.05)  # The height of the cells\n",
    "\n",
    "## Display the table ##\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Then test if the results are parametrical or not ##\n",
    "\n",
    "## Perform Shapiro-Wilk Test for Normality ##\n",
    "print(\"Shapiro-Wilk Test for Normality:\")\n",
    "for group_name, group_df in df.groupby('groups'):\n",
    "    stat, p = shapiro(group_df['scores'])\n",
    "    print(f\"{group_name}: W-statistic={stat:.4f}, p-value={p:.4f}\")\n",
    "\n",
    "## Perform Levene's Test for Homogeneity of Variances ##\n",
    "print(\"\\nLevene’s Test for Homogeneity of Variances:\")\n",
    "grouped_scores = [group_df['scores'].values for _, group_df in df.groupby('groups')]\n",
    "stat, p = levene(*grouped_scores)\n",
    "print(f\"Statistic={stat:.4f}, p-value={p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Then test if the results are due to seed randomness or not ##\n",
    "\n",
    "## Perform Kruskal-Wallis Test for each group separately ##\n",
    "grouped = df.groupby('groups')\n",
    "p_values = {}\n",
    "for group_name, group in grouped:\n",
    "    stat, p = kruskal(*[group[group['seeds'] == seed]['scores'] for seed in group['seeds'].unique()])\n",
    "    p_values[group_name] = p\n",
    "    print(f\"The Kruskal-Wallis test for {group_name}: p-value={p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform pairwise comparisons between every model with posthoc Dunn's ##\n",
    "\n",
    "## Ensure the groups are sorted in the correct order, same as before ##\n",
    "group_order = [\n",
    "    'female_under_18', 'female_18_to_49', 'female_over_49',\n",
    "    'male_under_18', 'male_18_to_49', 'male_over_49'\n",
    "]\n",
    "df['groups'] = pd.Categorical(df['groups'], categories=group_order, ordered=True)\n",
    "df.sort_values('groups', inplace=True)\n",
    "\n",
    "## Group the data by 'groups' for the Kruskal-Wallis test ##\n",
    "grouped_data = [group['scores'].values for name, group in df.groupby('groups')]\n",
    "\n",
    "## Perform the Kruskal-Wallis test ##\n",
    "stat, p = kruskal(*grouped_data)\n",
    "print('Kruskal-Wallis Test p-value:', p)\n",
    "\n",
    "## Perform posthoc test if the Kruskal-Wallis test is statistically significant ##\n",
    "if p < 0.05:\n",
    "    p_values = sp.posthoc_dunn(df, val_col='scores', group_col='groups', p_adjust='bonferroni')\n",
    "    \n",
    "    ## Define a color map for statistical significance ##\n",
    "    significance_level = 0.05\n",
    "    color_map = {}\n",
    "    \n",
    "    unique_p_vals = pd.Series(p_values.where(p_values <= significance_level).stack().unique()).sort_values()\n",
    "    colors = ['#006400', '#228B22', '#32CD32', '#00FF00', '#ADFF2F', '#90EE90']\n",
    "    \n",
    "    for i, p_val in enumerate(unique_p_vals):\n",
    "        if i < len(colors):\n",
    "            color_map[p_val] = colors[i]\n",
    "        else:\n",
    "            color_map[p_val] = colors[-1]\n",
    "    \n",
    "    ## Apply the color map to the p-values ##\n",
    "    color_applied = p_values.applymap(lambda x: color_map.get(x, 'white'))\n",
    "    \n",
    "    ## Format all values to three decimal places ##\n",
    "    formatted_values = p_values.applymap(lambda x: f\"{x:.8f}\" if x != 1 else \"1\")\n",
    "    \n",
    "    ## Add \"Model name\" to the column names ##\n",
    "    formatted_values.reset_index(inplace=True)\n",
    "    formatted_values.columns = ['Model name'] + list(formatted_values.columns[1:])\n",
    "    color_applied.reset_index(inplace=True)\n",
    "    color_applied.columns = ['Model name'] + list(color_applied.columns[1:])\n",
    "    \n",
    "    ## Set the color of the 'Model name' column to white ##\n",
    "    color_applied['Model name'] = 'white'\n",
    "    \n",
    "    ## Create a figure to display the table ##\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    ## Display the table ##\n",
    "    the_table = ax.table(cellText=formatted_values.values,\n",
    "                         colLabels=formatted_values.columns,\n",
    "                         cellLoc='center', \n",
    "                         loc='center',\n",
    "                         cellColours=color_applied.values)\n",
    "    \n",
    "    ## Adjust table scale ##\n",
    "    the_table.scale(0.7, 1.4)\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"The Kruskal-Wallis test is not statistically significant, no posthoc test performed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform pairwise comparisons between every model with posthoc Nemenyi ##\n",
    "\n",
    "## Ensure the groups are sorted in the correct order, same as before ##\n",
    "group_order = [\n",
    "    'female_under_18', 'female_18_to_49', 'female_over_49',\n",
    "    'male_under_18', 'male_18_to_49', 'male_over_49'\n",
    "]\n",
    "df['groups'] = pd.Categorical(df['groups'], categories=group_order, ordered=True)\n",
    "df.sort_values('groups', inplace=True)\n",
    "\n",
    "## Group the data by 'groups' for the Kruskal-Wallis test ##\n",
    "grouped_data = [group['scores'].values for name, group in df.groupby('groups')]\n",
    "\n",
    "## Perform the Kruskal-Wallis test ##\n",
    "stat, p = kruskal(*grouped_data)\n",
    "print('Kruskal-Wallis Test p-value:', p)\n",
    "\n",
    "## Perform posthoc test if the Kruskal-Wallis test is statistically significant ##\n",
    "if p < 0.05:\n",
    "    p_values = sp.posthoc_nemenyi(df, val_col='scores', group_col='groups')\n",
    "    \n",
    "    ## Define a color map for statistical significance ##\n",
    "    significance_level = 0.05\n",
    "    color_map = {}\n",
    "    \n",
    "    unique_p_vals = pd.Series(p_values.where(p_values <= significance_level).stack().unique()).sort_values()\n",
    "    colors = ['#006400', '#228B22', '#32CD32', '#00FF00', '#ADFF2F', '#90EE90']\n",
    "    \n",
    "    for i, p_val in enumerate(unique_p_vals):\n",
    "        if i < len(colors):\n",
    "            color_map[p_val] = colors[i]\n",
    "        else:\n",
    "            color_map[p_val] = colors[-1]\n",
    "    \n",
    "    ## Apply the color map to the p-values ##\n",
    "    color_applied = p_values.applymap(lambda x: color_map.get(x, 'white'))\n",
    "    \n",
    "    ## Format all values to three decimal places, including trailing zeros ##\n",
    "    formatted_values = p_values.applymap(lambda x: f\"{x:.8f}\" if x != 1 else \"1\")\n",
    "    \n",
    "    ## Add \"Model name\" to the column names ##\n",
    "    formatted_values.reset_index(inplace=True)\n",
    "    formatted_values.columns = ['Model name'] + list(formatted_values.columns[1:])\n",
    "    color_applied.reset_index(inplace=True)\n",
    "    color_applied.columns = ['Model name'] + list(color_applied.columns[1:])\n",
    "    \n",
    "    ## Set the color of the 'Model name' column to white ##\n",
    "    color_applied['Model name'] = 'white'\n",
    "    \n",
    "    ## Create a figure to display the table ##\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    ## Display the table ##\n",
    "    the_table = ax.table(cellText=formatted_values.values,\n",
    "                         colLabels=formatted_values.columns,\n",
    "                         cellLoc='center', \n",
    "                         loc='center',\n",
    "                         cellColours=color_applied.values)\n",
    "    \n",
    "    ## Adjust table scale ##\n",
    "    the_table.scale(0.7, 1.4)\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"The Kruskal-Wallis test is not statistically significant, no posthoc test performed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
